
# 环境搭建

### 准备 Parquet 测试数据


为了全面验证 parquetWin 插件对各种数据类型（如 INT64、DOUBLE、TIMESTAMP、STRING 等）的解析能力及分布式加载性能，需要准备一组标准的 Parquet 文件。 


- 基础数据：准备包含多种数据类型的 spark.parquet。 
- 特殊类型数据：包含 NANOTIMESTAMP 的 machine.parquet。 
- 索引数据：包含 Pandas 索引列（__index_level_0__）的 quotademo.parquet。 
- 大数据集：用于性能测试的大型 Parquet 文件（如 userdata1.parquet）。 

将这些文件放置在测试脚本指定的 DATA_DIR 路径下，例如：D:/plugins/DolphinDBPlugin/parquetWin/test/setup/data/。

### 启动 DolphinDB，运行回归用例

确保 DolphinDB Server 已启动，并将插件文件（PluginparquetWin.txt 和 .dll）放置在插件目录下。

根据实际测试环境修改 test_parquetwin.dos中数据目录DATA_DIR和工作目录WORK_DIR，例如；

```
DATA_DIR = "D:/plugins/DolphinDBPlugin/parquetWin/test/setup/data/"
WORK_DIR = "D:/tmp/parquet/"

```
修改test_parquetwin.dos中加载插件的路径,例如：
```
loadPlugin(getHomeDir()+"/plugins/parquetWin/PluginParquetWin.txt")
```
执行测试：
```

test("D:/plugins/DolphinDBPlugin/parquetWin/test/test_parquetwin.dos",
 "D:/test/test_demo_output.txt");
```

回归用例的结果将输出到指定的文件中。



# 测试要点
本插件包含 Schema 提取、内存加载、分区加载、数据源生成及 Parquet 文件保存五大核心功能。以下是详细测试要点：

### 1. extractParquetSchema

接口：parquetWin::extractParquetSchema(fileName)  
功能：提取 Parquet 文件的元数据结构。


异常用例： 

- 文件不存在：路径错误。

- 参数缺失：未输入文件名。

- 非法参数类型：输入向量、矩阵、字典、集合或表而非字符串标量。

- 参数过多：输入多于 1 个参数。


正常用例： 

- 提取标准文件：验证返回表的列名（name）与类型（type）是否符合预期。

- 重复调用：验证连续调用 100 次的稳定性。 

### 2. loadParquet

接口：parquetWin::loadParquet(fileName, [schema], [columnsToRead], [rowGroupStart], [rowGroupNum])  
功能：将 Parquet 文件加载为 DolphinDB 内存表。


异常用例： 

- 参数校验：schema 非表、columnsToRead 为标量或非整型向量、rowGroupStart 为负数。

- 列索引越界：columnsToRead 包含不存在的索引。


正常用例： 

- 类型转换：通过 schema 参数将 String 映射为 SYMBOL，或将 Date 映射为 MONTH。

- 部分加载：通过 columnsToRead 指定加载特定列；通过 rowGroupStart 和 rowGroupNum 加载特定行组。

- Pandas 索引：支持加载并重命名 Pandas 生成的索引列。 


### 3. loadParquetEx

接口：parquetWin::loadParquetEx(dbHandle, tableName, partitionColumns, fileName, [schema], [columns], [rgStart], [rgNum], [transform])  
功能：将 Parquet 数据直接加载到分区数据库中。

异常用例： 

- 数据库句柄异常：dbHandle 为 NULL 或整型标量。

- 分区列错误：partitionColumns 不存在于文件中，或在简单分区下输入多个分区列。

- Schema 冲突：提供的 schema 表与文件实际结构或数据库分区方案不匹配。

- 不支持的分区：尝试加载到 SEQ 类型的数据库。 


正常用例： 

- 多分区方案：验证 VALUE 和 RANGE 分区下的加载正确性。

- Transform 预处理：加载时通过自定义函数（如 sum 或数值缩放）对列数据进行转换。

- 分布式存储（DFS）：验证数据成功写入 dfs:// 开头的分布式数据库。 

### 4. parquetDS

接口：parquetWin::parquetDS(fileName, [schema])  
功能：为 Parquet 文件生成数据源列表，支持 MapReduce 操作。


异常用例： 

- 文件名非法：输入整型或向量。

- Schema 非法：输入非法类型或错误的列定义（如更新类型为 KK）。


正常用例： 


- 基础 MapReduce：结合 mr 函数验证数据源的读取完整性。

- 并发数据源：在多 Job 环境下生成并消费数据源。 

### 5. saveParquet

接口：parquetWin::saveParquet(table, fileName, [compressMethod])  
功能：将 DolphinDB 表保存为 Parquet 文件。



异常用例： 

- 非法对象：保存非表对象（如整型标量或向量）。

- 参数过多：输入超过限定数量的参数。


正常用例： 

- 全空表与全 NULL 表：验证保存后的文件结构是否能被再次正确解析。

- 高精度类型：保存包含 NANOTIMESTAMP 和 NANOTIME 的表。 


- 大数据保存：验证百万级行数（5,000,000行）表的保存稳定性。 

### 6. 稳定性与并发测试 (通用要点) 

- 并发 Job 测试：使用 submitJob 同时发起多个 loadParquetEx 或 saveParquet 任务，验证是否存在资源竞态或内存崩溃。

- 多轮迭代测试：通过 for 循环执行提取和加载操作 100 次以上，观察 DolphinDB 的内存使用曲线，确保无内存泄漏（Memory Leak）。

- 大规模数据读写：验证在处理 5GB+ 以上数据量时的插件稳定性。



